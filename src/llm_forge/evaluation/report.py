"""Evaluation report generation (Markdown + CSV)."""

import csv
import json
import logging
from pathlib import Path
from typing import Any

logger = logging.getLogger(__name__)


def generate_report(
    results: dict,
    predictions: list[dict],
    true_labels: list[dict],
    output_dir: str,
) -> str:
    """Generate evaluation report in Markdown and CSV.

    Args:
        results: Metrics dict from compute_metrics.
        predictions: List of prediction dicts.
        true_labels: List of true label dicts.
        output_dir: Directory to save report files.

    Returns:
        Path to the Markdown report file.
    """
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)

    # Save predictions CSV
    csv_path = output_path / "predictions.csv"
    _save_predictions_csv(predictions, true_labels, csv_path)

    # Generate Markdown report
    md_path = output_path / "eval_report.md"
    _generate_markdown(results, md_path)

    logger.info("Report saved: %s", md_path)
    return str(md_path)


def _save_predictions_csv(
    predictions: list[dict],
    true_labels: list[dict],
    path: Path,
) -> None:
    """Save predictions with ground truth to CSV.

    Args:
        predictions: List of prediction dicts.
        true_labels: List of true label dicts.
        path: Output CSV path.
    """
    if not predictions:
        return

    # Collect all possible columns from true labels
    all_cols: set[str] = set()
    for label in true_labels:
        all_cols.update(label.keys())
    sorted_cols = sorted(all_cols)

    fieldnames = (
        ["input", "raw_output", "parse_success"]
        + [f"true_{col}" for col in sorted_cols]
        + [f"pred_{col}" for col in sorted_cols]
    )

    with open(path, "w", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()

        for pred, true in zip(predictions, true_labels):
            row = {
                "input": pred.get("input", ""),
                "raw_output": pred.get("raw_output", ""),
                "parse_success": pred.get("parse_success", False),
            }
            for col in sorted_cols:
                row[f"true_{col}"] = true.get(col, "")
                parsed = pred.get("parsed") or {}
                row[f"pred_{col}"] = parsed.get(col, "")

            writer.writerow(row)

    logger.info("Predictions saved: %s (%d rows)", path, len(predictions))


def _generate_markdown(results: dict, path: Path) -> None:
    """Generate Markdown evaluation report.

    Args:
        results: Metrics dict.
        path: Output Markdown path.
    """
    lines = [
        "# Evaluation Report",
        "",
        "## Summary",
        "",
        f"- **Total samples:** {results.get('total', 0)}",
        f"- **JSON parse rate:** {results.get('json_parse_rate', 0):.1%}",
        f"- **Overall accuracy:** {results.get('overall_accuracy', 0):.1%}",
        "",
    ]

    # Per-column metrics
    per_column = results.get("per_column", {})
    if per_column:
        lines.append("## Per-Column Metrics")
        lines.append("")

        for col_name, col_data in per_column.items():
            lines.append(f"### {col_name}")
            lines.append("")
            lines.append(
                f"- **Accuracy:** {col_data.get('accuracy', 0):.1%} "
                f"({col_data.get('correct', 0)}/{col_data.get('total', 0)})"
            )
            lines.append("")

            # Per-class table
            per_class = col_data.get("per_class", {})
            if per_class:
                lines.append("| Class | Accuracy | Correct | Total |")
                lines.append("|-------|----------|---------|-------|")
                for cls_name, cls_data in sorted(per_class.items()):
                    acc = cls_data.get("accuracy", 0)
                    correct = cls_data.get("correct", 0)
                    total = cls_data.get("count", 0)
                    lines.append(
                        f"| {cls_name} | {acc:.1%} | {correct} | {total} |"
                    )
                lines.append("")

    # Confusion matrix
    confusion = results.get("confusion_matrix", {})
    if confusion:
        labels = confusion.get("labels", [])
        matrix = confusion.get("matrix", [])
        if labels and matrix:
            lines.append("## Confusion Matrix")
            lines.append("")
            header = "| |" + "|".join(f" {l} " for l in labels) + "|"
            lines.append(header)
            separator = "|---|" + "|".join("---" for _ in labels) + "|"
            lines.append(separator)
            for i, label in enumerate(labels):
                row_str = f"| **{label}** |"
                row_str += "|".join(
                    f" {matrix[i][j]} " for j in range(len(labels))
                )
                row_str += "|"
                lines.append(row_str)
            lines.append("")

    lines.append("---")
    lines.append("*Generated by llm-forge*")

    with open(path, "w", encoding="utf-8") as f:
        f.write("\n".join(lines))
