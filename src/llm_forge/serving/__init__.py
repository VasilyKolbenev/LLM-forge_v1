"""Model serving: llama.cpp and vLLM backends."""
